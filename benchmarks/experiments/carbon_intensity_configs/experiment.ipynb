{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "from unittest.mock import patch\n",
    "import boto3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define locations\n",
    "locations = {\n",
    "    \"us-west-2\": (43.8041334, -120.5542012),\n",
    "    \"us-west-1\": (38.8375215, -120.8958242),\n",
    "}\n",
    "\n",
    "start_date = datetime(2023, 6, 1, 0, 0, 0)\n",
    "\n",
    "end_date = datetime(2023, 6, 30, 0, 0, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Carbon Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data combined and saved to ./data/carbon/us-west-2_carbon_data.json\n",
      "Data combined and saved to ./data/carbon/us-west-1_carbon_data.json\n"
     ]
    }
   ],
   "source": [
    "# Function to query the API\n",
    "def query_api(start, end, lat, lon, token):\n",
    "    url = f\"https://api-access.electricitymaps.com/free-tier/carbon-intensity/past-range?lat={lat}&lon={lon}&start={start}&end={end}\"\n",
    "    headers = {\"auth-token\": token}\n",
    "    response = requests.get(url, headers=headers, timeout=10)\n",
    "    if response.status_code == 200:\n",
    "        return response.json()[\"data\"]\n",
    "    else:\n",
    "        print(f\"Error querying API: {response.status_code}\")\n",
    "        return []\n",
    "\n",
    "def run():\n",
    "    for location, (latitude, longitude) in locations.items():\n",
    "        current_start = start_date - timedelta(days=7) # Start 7 days before the start date\n",
    "        combined_data = []\n",
    "\n",
    "        while current_start < end_date:\n",
    "            current_end = current_start + timedelta(days=10)\n",
    "            if current_end > end_date:\n",
    "                current_end = end_date\n",
    "            data = query_api(\n",
    "                current_start, current_end, latitude, longitude, os.environ.get(\"ELECTRICITY_MAPS_AUTH_TOKEN\")\n",
    "            )\n",
    "            combined_data.extend(data)\n",
    "            current_start = current_end + timedelta(days=1)\n",
    "\n",
    "        # Save the combined data to a file\n",
    "        output_file = f\"./data/carbon/{location}_carbon_data.json\"\n",
    "        with open(output_file, \"w\") as f:\n",
    "            json.dump(combined_data, f)\n",
    "\n",
    "        print(f\"Data combined and saved to {output_file}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Carbon Collector Logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "from collections import defaultdict\n",
    "\n",
    "def calculate_distance(lat1, lon1, lat2, lon2):\n",
    "    r = 6371.0  # Earth radius in kilometers\n",
    "    dlat = math.radians(lat2 - lat1)\n",
    "    dlon = math.radians(lon2 - lon1)\n",
    "    a = math.sin(dlat / 2)**2 + math.cos(math.radians(lat1)) * math.cos(math.radians(lat2)) * math.sin(dlon / 2)**2\n",
    "    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))\n",
    "    return r * c\n",
    "\n",
    "def process_and_store_carbon_data_for_regions(input_file, output_folder, regions_info, current_region):\n",
    "    os.makedirs(output_folder, exist_ok=True)  # Ensure output directory exists\n",
    "    \n",
    "    with open(input_file, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    \n",
    "    # For each day between start_date and end_date\n",
    "    for day in range((end_date - start_date).days + 1):\n",
    "        current_date = start_date + timedelta(days=day)\n",
    "\n",
    "        # Get all the data for the previous 7 days\n",
    "        previous_7_days_data = [entry for entry in data if datetime.strptime(entry['datetime'], '%Y-%m-%dT%H:%M:%S.%fZ').date() >= (current_date - timedelta(days=7)).date() and datetime.strptime(entry['datetime'], '%Y-%m-%dT%H:%M:%S.%fZ').date() < current_date.date()]\n",
    "\n",
    "        # Calculate the average carbon intensity for the previous 7 days\n",
    "        if not previous_7_days_data:\n",
    "            continue\n",
    "        overall_sum = sum(item['carbonIntensity'] for item in previous_7_days_data)\n",
    "        overall_avg = overall_sum / len(previous_7_days_data)\n",
    "\n",
    "        # Calculate the average carbon intensity for each hour of the day\n",
    "        hourly_averages = defaultdict(list)\n",
    "        for item in previous_7_days_data:\n",
    "            hour = datetime.strptime(item['datetime'], '%Y-%m-%dT%H:%M:%S.%fZ').hour\n",
    "            hourly_averages[hour].append(item['carbonIntensity'])\n",
    "\n",
    "        hourly_avg = {hour: sum(values) / len(values) for hour, values in hourly_averages.items()}\n",
    "\n",
    "        # Calculate the distances between the regions\n",
    "        transmission_distances = {\n",
    "            f\"aws:{region_key}\": calculate_distance(regions_info[current_region][0], regions_info[current_region][1], regions_info[region_key][0], regions_info[region_key][1]) for region_key in regions_info\n",
    "        }\n",
    "    \n",
    "        # Assemble the result dictionary\n",
    "        result_dict = {\n",
    "            \"averages\": {\n",
    "                \"overall\": {\"carbon_intensity\": overall_avg},\n",
    "                **{str(hour): {\"carbon_intensity\": avg} for hour, avg in hourly_avg.items()}\n",
    "            },\n",
    "            \"units\": \"gCO2eq/kWh\",\n",
    "            \"transmission_distances\": transmission_distances\n",
    "        }\n",
    "\n",
    "        # Store the result\n",
    "        day_folder = os.path.join(output_folder, current_date.strftime('%Y-%m-%d'))\n",
    "        os.makedirs(day_folder, exist_ok=True)\n",
    "        with open(os.path.join(day_folder, 'data.json'), 'w') as outfile:\n",
    "            json.dump(result_dict, outfile, indent=4)\n",
    "\n",
    "for region in locations:\n",
    "    process_and_store_carbon_data_for_regions(f\"./data/carbon/{region}_carbon_data.json\", f\"./data/collected_carbon/{region}\", locations, region)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Data from the Home Region\n",
    "\n",
    "For each benchmark:\n",
    "1. Deploy the benchmark (optimally without any constraints) to the home region (which should be one of the regions we eventually also want to take into consideration).\n",
    "2. Run the benchmark for a time (use the `invoke_serverless_function_uniform.sh` script to do so). I used it for example like this:\n",
    "```bash\n",
    "poetry run ./invoke_serverless_function_uniform.sh dna_visualization 0.0.1 200 3600\n",
    "```\n",
    "3. Use the following small script to log sync the data and store it locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the benchmarks:\n",
    "benchmarks = [\"dna_visualization-0.0.1\"]\n",
    "\n",
    "benchmark_input_data = {\n",
    "    \"dna_visualization-0.0.1\": '{\"gen_file_name\": \"small_sequence.gb\"}'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Log Sync"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multi_x_serverless.syncers.log_syncer import LogSyncWorkflow\n",
    "from multi_x_serverless.common.models.endpoints import Endpoints\n",
    "from multi_x_serverless.common.constants import DEPLOYMENT_MANAGER_RESOURCE_TABLE, GLOBAL_TIME_ZONE\n",
    "\n",
    "region_clients = {}\n",
    "endpoints = Endpoints()\n",
    "\n",
    "# This assumes that the run happened in the last 24 hours\n",
    "time_intervals = [(datetime.now(GLOBAL_TIME_ZONE) - timedelta(days=1), datetime.now(GLOBAL_TIME_ZONE))]\n",
    "\n",
    "def log_sync(output_dir, workflow_id):\n",
    "    deployment_manager_config_str = endpoints.get_deployment_manager_client().get_value_from_table(\n",
    "        DEPLOYMENT_MANAGER_RESOURCE_TABLE, workflow_id\n",
    "    )\n",
    "\n",
    "    raw_data_dir = output_dir\n",
    "    os.makedirs(raw_data_dir, exist_ok=True)\n",
    "\n",
    "    def mock_upload_data(self, data_for_upload: str):\n",
    "        with open(f\"{raw_data_dir}/data.json\", \"w\") as f:\n",
    "            f.write(data_for_upload)\n",
    "\n",
    "    with patch.object(LogSyncWorkflow, \"_upload_data\", mock_upload_data):\n",
    "        log_sync_workflow = LogSyncWorkflow(\n",
    "            workflow_id,\n",
    "            region_clients,\n",
    "            deployment_manager_config_str,\n",
    "            time_intervals,\n",
    "            endpoints.get_datastore_client(),\n",
    "            {}\n",
    "        )\n",
    "        log_sync_workflow.sync_workflow()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Invoke Log Sync"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for workflow_id in benchmarks:\n",
    "    log_sync(f\"./data/run_data/home_region/{workflow_id}\", workflow_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Workflow Collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multi_x_serverless.data_collector.components.workflow.workflow_retriever import WorkflowRetriever\n",
    "from multi_x_serverless.data_collector.components.data_collector import DataCollector\n",
    "\n",
    "workflow_retriever = WorkflowRetriever(None)\n",
    "\n",
    "def workflow_collect(input_dir, output_dir):\n",
    "    with open(input_dir, \"r\") as f:\n",
    "        data = f.read()\n",
    "\n",
    "    workflow_summary = workflow_retriever._transform_workflow_summary(\n",
    "        data,\n",
    "    )\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    with open(f\"{output_dir}/data.json\", \"w\") as f:\n",
    "        json.dump(workflow_summary, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Invoke Workflow Collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for workflow_id in benchmarks:\n",
    "    workflow_collect(f\"./data/run_data/home_region/{workflow_id}/data.json\", f\"./data/collected_workflow/{workflow_id}/home_region\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the Deployment Algorithm\n",
    "\n",
    "We need to run the deployment algorithm for the days between the start and end date. We will run it every day and store the results locally in files.\n",
    "Additionally, we need to provide the data collected by a specific workflow as the input to the workflow loader.\n",
    "\n",
    "**TODO BEFORE the following is executed:** For each of the involved benchmarks we need to actually have some data from the home region. So please do **Get Data from the Home Region** first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multi_x_serverless.common.constants import GLOBAL_SYSTEM_REGION, DEPLOYMENT_OPTIMIZATION_MONITOR_RESOURCE_TABLE\n",
    "from multi_x_serverless.routing.workflow_config import WorkflowConfig\n",
    "from multi_x_serverless.routing.deployment_algorithms.stochastic_heuristic_deployment_algorithm import StochasticHeuristicDeploymentAlgorithm\n",
    "from multi_x_serverless.routing.deployment_input.components.loaders.carbon_loader import CarbonLoader\n",
    "from multi_x_serverless.routing.deployment_input.components.loaders.region_viability_loader import RegionViabilityLoader\n",
    "from multi_x_serverless.routing.deployment_algorithms.deployment_algorithm import DeploymentAlgorithm\n",
    "from multi_x_serverless.routing.deployment_input.components.loaders.workflow_loader import WorkflowLoader\n",
    "\n",
    "# Define the constraint configurations\n",
    "constraint_configurations = {\n",
    "    \"no_constraints\": {\n",
    "        \"constraints\": {\n",
    "            \"hard_resource_constraints\": {\"cost\": None, \"runtime\": None, \"carbon\": None},\n",
    "            \"soft_resource_constraints\": {\"cost\": None, \"runtime\": None, \"carbon\": None},\n",
    "            \"priority_order\": [\"carbon\", \"cost\", \"runtime\"],\n",
    "        }\n",
    "    },\n",
    "    \"five_percent_runtime_constraint\": {\n",
    "        \"constraints\": {\n",
    "            \"hard_resource_constraints\": {\"cost\": None, \"runtime\": {\"type\": \"relative\", \"value\": 105}, \"carbon\": None},\n",
    "            \"soft_resource_constraints\": {\"cost\": None, \"runtime\": None, \"carbon\": None},\n",
    "            \"priority_order\": [\"carbon\", \"cost\", \"runtime\"],\n",
    "        }\n",
    "    },\n",
    "}\n",
    "\n",
    "dynamodb_client = boto3.client('dynamodb', region_name=GLOBAL_SYSTEM_REGION)\n",
    "for benchmark in benchmarks:\n",
    "    response = dynamodb_client.get_item(TableName=DEPLOYMENT_OPTIMIZATION_MONITOR_RESOURCE_TABLE, Key={\"key\": {\"S\": benchmark}})\n",
    "    item = response.get(\"Item\")\n",
    "    workflow_config_from_table = item[\"value\"][\"S\"]\n",
    "\n",
    "    workflow_config_dict = json.loads(json.loads(workflow_config_from_table).get(\"workflow_config\"))\n",
    "    for configuration_name, configuration in constraint_configurations.items():\n",
    "        workflow_config_dict[\"constraints\"] = configuration[\"constraints\"]\n",
    "        workflow_config = WorkflowConfig(workflow_config_dict)\n",
    "\n",
    "        for day in range((end_date - start_date).days + 1):\n",
    "            current_date = start_date + timedelta(days=day)\n",
    "\n",
    "            def mock_carbon_loader_setup(self, available_regions: set[str]):\n",
    "                real_available_regions = list(locations.keys())\n",
    "                loaded_carbon_data = {}\n",
    "                for region in real_available_regions:\n",
    "                    carbon_data_path = f'./data/collected_carbon/{region}/{current_date.strftime(\"%Y-%m-%d\")}/data.json'\n",
    "                    with open(carbon_data_path, 'r') as file:\n",
    "                        loaded_carbon_data[f'aws:{region}'] = json.load(file)\n",
    "                self._carbon_data = loaded_carbon_data\n",
    "\n",
    "            def mock_workflow_loader_setup(self, workflow_id: str):\n",
    "                collected_data_dir = f\"./data/collected_workflow/{workflow_id}/home_region/data.json\"\n",
    "\n",
    "                with open(collected_data_dir, \"r\") as f:\n",
    "                    self._workflow_data = json.load(f)\n",
    "\n",
    "            def mock_region_viability_setup(self):\n",
    "                self._available_regions = [f'aws:{region}' for region in locations.keys()]\n",
    "\n",
    "            def mock_upload_result(self, result: dict):\n",
    "                result_path = f'./data/deployment_results/{benchmark}/{configuration_name}/'\n",
    "                os.makedirs(result_path, exist_ok=True)\n",
    "                with open(os.path.join(result_path, f'{current_date.strftime(\"%Y-%m-%d\")}.json'), 'w') as file:\n",
    "                    json.dump(result, file, indent=4)\n",
    "\n",
    "            with patch.object(CarbonLoader, 'setup', new=mock_carbon_loader_setup), patch.object(RegionViabilityLoader, 'setup', new=mock_region_viability_setup), patch.object(DeploymentAlgorithm, '_upload_result', new=mock_upload_result), patch.object(WorkflowLoader, 'setup', new=mock_workflow_loader_setup):\n",
    "                algorithm = StochasticHeuristicDeploymentAlgorithm(workflow_config)\n",
    "                algorithm.run([f\"{i}\" for i in range(24)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run new experiments for the new deployments\n",
    "\n",
    "Now since we have the new deployments, we can run the experiments for them. We don't need to run all experiments, basically we need to run all the combinations of the regions involved. For this we need to iterate over the potential deployments, find combinations of regions that are not yet collected and run the experiments for them. The following script will do this on a per benchmark basis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose which benckmark to run\n",
    "workflow_id = benchmarks[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get unique new deployments for this benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment_result_path = f'./data/deployment_results/{workflow_id}'\n",
    "\n",
    "unique_instance_region_combinations = set()\n",
    "\n",
    "# Iterate over all results, retrieving the combinations of instances and regions\n",
    "for directory in os.listdir(deployment_result_path):\n",
    "    if not os.path.isdir(os.path.join(deployment_result_path, directory)):\n",
    "        continue\n",
    "    for file in os.listdir(os.path.join(deployment_result_path, directory)):\n",
    "        with open(os.path.join(deployment_result_path, directory, file), 'r') as f:\n",
    "            data = json.load(f)\n",
    "        for time_key, deployment in data[\"time_keys_to_staging_area_data\"].items():\n",
    "            deployment_str = json.dumps(deployment)\n",
    "            unique_instance_region_combinations.add(deployment_str)\n",
    "\n",
    "# Store the unique combinations\n",
    "with open(f'./data/deployment_results/{workflow_id}/unique_instance_region_combinations.json', 'w') as f:\n",
    "    json.dump(list(unique_instance_region_combinations), f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Re-deploy all deployments\n",
    "\n",
    "Before we can run the experiments we need the code to be in the right regions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found credentials in shared credentials file: ~/.aws/credentials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found credentials in shared credentials file: ~/.aws/credentials\n",
      "Found credentials in shared credentials file: ~/.aws/credentials\n",
      "Found credentials in shared credentials file: ~/.aws/credentials\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dna_visualization-0_0_1-GetInput_aws-us-west-1': {'deploy_region': {'provider': 'aws', 'region': 'us-west-1'}, 'message_topic': 'arn:aws:sns:us-west-1:485595969306:dna_visualization-0_0_1-GetInput_aws-us-west-1_messaging_topic', 'function_identifier': 'arn:aws:lambda:us-west-1:485595969306:function:dna_visualization-0_0_1-GetInput_aws-us-west-1'}, 'dna_visualization-0_0_1-Visualize_aws-us-west-1': {'deploy_region': {'provider': 'aws', 'region': 'us-west-1'}, 'message_topic': 'arn:aws:sns:us-west-1:485595969306:dna_visualization-0_0_1-Visualize_aws-us-west-1_messaging_topic', 'function_identifier': 'arn:aws:lambda:us-west-1:485595969306:function:dna_visualization-0_0_1-Visualize_aws-us-west-1'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found credentials in shared credentials file: ~/.aws/credentials\n",
      "Found credentials in shared credentials file: ~/.aws/credentials\n",
      "2024/03/15 11:39:35 logged in via /Users/viktorgsteiger/.docker/config.json\n",
      "2024/03/15 11:39:35 Copying from 485595969306.dkr.ecr.us-west-1.amazonaws.com/dna_visualization-0_0_1-visualize_aws-us-west-1:latest to 485595969306.dkr.ecr.us-west-2.amazonaws.com/dna_visualization-0_0_1-visualize_aws-us-west-2:latest\n",
      "2024/03/15 11:39:36 existing manifest: latest@sha256:fcbebf5f8d5d8fee733e1a4ce309f4a2c2cdbc77e61e05f4a67ec01e3dbaa1d3\n",
      "Docker image 485595969306.dkr.ecr.us-west-2.amazonaws.com/dna_visualization-0_0_1-visualize_aws-us-west-2:latest copied successfully.\n",
      "Error while updating function configuration: An error occurred (ResourceConflictException) when calling the UpdateFunctionConfiguration operation: The operation cannot be performed at this time. An update is in progress for resource: arn:aws:lambda:us-west-2:485595969306:function:dna_visualization-0_0_1-Visualize_aws-us-west-2\n",
      "Found credentials in shared credentials file: ~/.aws/credentials\n",
      "Found credentials in shared credentials file: ~/.aws/credentials\n",
      "2024/03/15 11:39:46 logged in via /Users/viktorgsteiger/.docker/config.json\n",
      "2024/03/15 11:39:46 Copying from 485595969306.dkr.ecr.us-west-1.amazonaws.com/dna_visualization-0_0_1-getinput_aws-us-west-1:latest to 485595969306.dkr.ecr.us-west-2.amazonaws.com/dna_visualization-0_0_1-getinput_aws-us-west-2:latest\n",
      "2024/03/15 11:39:47 existing manifest: latest@sha256:e5a94e0a71713eb4187130c44b58bbd883714a10d8582149409e8686bfde1468\n",
      "Docker image 485595969306.dkr.ecr.us-west-2.amazonaws.com/dna_visualization-0_0_1-getinput_aws-us-west-2:latest copied successfully.\n",
      "Error while updating function configuration: An error occurred (ResourceConflictException) when calling the UpdateFunctionConfiguration operation: The operation cannot be performed at this time. An update is in progress for resource: arn:aws:lambda:us-west-2:485595969306:function:dna_visualization-0_0_1-GetInput_aws-us-west-2\n"
     ]
    }
   ],
   "source": [
    "from multi_x_serverless.deployment.common.deploy.deployer import create_default_deployer\n",
    "from multi_x_serverless.common.constants import (\n",
    "    DEPLOYMENT_MANAGER_RESOURCE_TABLE\n",
    ")\n",
    "from multi_x_serverless.deployment.server.re_deployment_server import ReDeploymentServer\n",
    "from multi_x_serverless.deployment.common.factories.deployer_factory import DeployerFactory\n",
    "from multi_x_serverless.common.models.endpoints import Endpoints\n",
    "\n",
    "endpoints = Endpoints()\n",
    "\n",
    "deployment_result_path = f'./data/deployment_results/{workflow_id}/unique_instance_region_combinations.json'\n",
    "\n",
    "workflow_data_raw = endpoints.get_deployment_manager_client().get_value_from_table(\n",
    "    DEPLOYMENT_MANAGER_RESOURCE_TABLE, workflow_id\n",
    ")\n",
    "\n",
    "workflow_data = json.loads(workflow_data_raw)\n",
    "\n",
    "workflow_function_descriptions = json.loads(workflow_data[\"workflow_function_descriptions\"])\n",
    "deployment_config = json.loads(workflow_data[\"deployment_config\"])\n",
    "deployed_regions = json.loads(workflow_data[\"deployed_regions\"])\n",
    "\n",
    "deployer_factory = DeployerFactory(project_dir=None)\n",
    "config = deployer_factory.create_config_obj_from_dict(deployment_config=deployment_config)\n",
    "deployer = create_default_deployer(config)\n",
    "\n",
    "re_redployment_server = ReDeploymentServer(workflow_id)\n",
    "\n",
    "with open(deployment_result_path, 'r') as f:\n",
    "    unique_instance_region_combinations = json.load(f)\n",
    "\n",
    "for combination in unique_instance_region_combinations:\n",
    "    deployment = json.loads(combination)\n",
    "\n",
    "    deployer.re_deploy(\n",
    "        workflow_function_descriptions,\n",
    "        deployed_regions,\n",
    "        deployment\n",
    "    )\n",
    "\n",
    "re_redployment_server._upload_new_deployed_regions(deployed_regions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run actual experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_invocations = 100\n",
    "time_frame_seconds = 60 * 30 # 30 minutes\n",
    "\n",
    "# Calculate the sleep time between each invocation\n",
    "sleep_time = time_frame_seconds / number_of_invocations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multi_x_serverless.common.models.endpoints import Endpoints\n",
    "from multi_x_serverless.common.constants import (\n",
    "    WORKFLOW_PLACEMENT_DECISION_TABLE,\n",
    "    DEPLOYMENT_MANAGER_RESOURCE_TABLE,\n",
    "    TIME_FORMAT,\n",
    "    GLOBAL_TIME_ZONE\n",
    ")\n",
    "from multi_x_serverless.endpoint.client import Client\n",
    "from multi_x_serverless.common.models.remote_client.remote_client_factory import RemoteClientFactory\n",
    "import time\n",
    "\n",
    "deployment_result_path = f'./data/deployment_results/{workflow_id}/unique_instance_region_combinations.json'\n",
    "\n",
    "with open(deployment_result_path, 'r') as f:\n",
    "    unique_instance_region_combinations = json.load(f)\n",
    "\n",
    "endpoints = Endpoints()\n",
    "\n",
    "deployed_region_information = endpoints.get_deployment_manager_client().get_value_from_table(\n",
    "    DEPLOYMENT_MANAGER_RESOURCE_TABLE, workflow_id\n",
    ")\n",
    "\n",
    "deployed_regions = json.loads(json.loads(deployed_region_information)[\"deployed_regions\"])\n",
    "\n",
    "current_workflow_placement_raw = endpoints.get_deployment_algorithm_workflow_placement_decision_client().get_value_from_table(\n",
    "    WORKFLOW_PLACEMENT_DECISION_TABLE, workflow_id\n",
    ")\n",
    "\n",
    "current_workflow_placement = json.loads(current_workflow_placement_raw)\n",
    "\n",
    "current_workflow_placement[\"send_to_home_region\"] = True\n",
    "\n",
    "client = Client(workflow_id)\n",
    "\n",
    "def instance_to_function_name(provider_region, instance_name):\n",
    "    return (\n",
    "        instance_name.split(\":\", maxsplit=1)[0]\n",
    "        + \"_\"\n",
    "        + provider_region[\"provider\"]\n",
    "        + \"-\"\n",
    "        + provider_region[\"region\"]\n",
    "    )\n",
    "\n",
    "for combination_raw in unique_instance_region_combinations:\n",
    "    deployment = json.loads(combination_raw)\n",
    "\n",
    "    # We need to override the home_deployment:\n",
    "    for instance_name, instance_description in current_workflow_placement[\"workflow_placement\"][\"home_deployment\"].items():\n",
    "        instance_description[\"provider_region\"] = deployment[instance_name][\"provider_region\"]\n",
    "        function_name = instance_to_function_name(\n",
    "            instance_description[\"provider_region\"], instance_name\n",
    "        )\n",
    "        messaging_queue_identifier = deployed_regions[function_name][\"message_topic\"]\n",
    "        function_identifier = deployed_regions[function_name][\"function_identifier\"]\n",
    "\n",
    "        instance_description[\"identifier\"] = messaging_queue_identifier\n",
    "        instance_description[\"function_identifier\"] = function_identifier\n",
    "\n",
    "    provider, region, identifier = client._get_initial_node_workflow_placement_decision(\n",
    "        current_workflow_placement, True\n",
    "    )\n",
    "\n",
    "    current_time = datetime.now(GLOBAL_TIME_ZONE).strftime(TIME_FORMAT)\n",
    "\n",
    "    wrapped_input_data = {\n",
    "        \"input_data\": benchmark_input_data[workflow_id],\n",
    "        \"time_request_sent\": current_time,\n",
    "        \"workflow_placement_decision\": current_workflow_placement,\n",
    "    }\n",
    "\n",
    "    json_payload = json.dumps(wrapped_input_data)\n",
    "\n",
    "    remote_client = RemoteClientFactory.get_remote_client(provider, region)\n",
    "\n",
    "    for i in range(number_of_invocations):\n",
    "        remote_client.invoke_function(\n",
    "            message=json_payload,\n",
    "            identifier=identifier,\n",
    "        )\n",
    "        time.sleep(sleep_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replay results on carbon data\n",
    "\n",
    "Now we have all the pieces we need. We have the deployments for a specific day under the given constraints, we have the carbon data for the same day and we have the runtime results for the specific deployments. Now we can replay the results on the carbon data and see how much carbon we would have emitted if we had deployed the functions in the way we did."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First we need to log sync that data into a new file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments_collected_data_dir = f\"./data/collected_workflow/{workflow_id}/experiments\"\n",
    "\n",
    "log_sync(f\"./data/run_data/experiments/{workflow_id}\", workflow_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for -: 'set' and 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m experiments_collected_data_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./data/collected_workflow/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mworkflow_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/experiments\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mworkflow_collect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./data/run_data/experiments/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mworkflow_id\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/data.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexperiments_collected_data_dir\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[32], line 10\u001b[0m, in \u001b[0;36mworkflow_collect\u001b[0;34m(input_dir, output_dir)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(input_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      8\u001b[0m     data \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m---> 10\u001b[0m workflow_summary \u001b[38;5;241m=\u001b[39m \u001b[43mworkflow_retriever\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_transform_workflow_summary\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(output_dir, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/data.json\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "File \u001b[0;32m~/Documents/multi-x-serverless/multi_x_serverless/data_collector/components/workflow/workflow_retriever.py:28\u001b[0m, in \u001b[0;36mWorkflowRetriever._transform_workflow_summary\u001b[0;34m(self, workflow_summarized)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_transform_workflow_summary\u001b[39m(\u001b[38;5;28mself\u001b[39m, workflow_summarized: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[1;32m     26\u001b[0m     summarized_workflow \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(workflow_summarized)\n\u001b[0;32m---> 28\u001b[0m     start_hop_summary, instance_summary \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_construct_summaries\u001b[49m\u001b[43m(\u001b[49m\u001b[43msummarized_workflow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m     31\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mworkflow_runtime_samples\u001b[39m\u001b[38;5;124m\"\u001b[39m: summarized_workflow[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mworkflow_runtime_samples\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     32\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdaily_invocation_counts\u001b[39m\u001b[38;5;124m\"\u001b[39m: summarized_workflow\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdaily_invocation_counts\u001b[39m\u001b[38;5;124m\"\u001b[39m, {}),\n\u001b[1;32m     33\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstart_hop_summary\u001b[39m\u001b[38;5;124m\"\u001b[39m: start_hop_summary,\n\u001b[1;32m     34\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minstance_summary\u001b[39m\u001b[38;5;124m\"\u001b[39m: instance_summary,\n\u001b[1;32m     35\u001b[0m     }\n",
      "File \u001b[0;32m~/Documents/multi-x-serverless/multi_x_serverless/data_collector/components/workflow/workflow_retriever.py:43\u001b[0m, in \u001b[0;36mWorkflowRetriever._construct_summaries\u001b[0;34m(self, logs)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m log \u001b[38;5;129;01min\u001b[39;00m logs:\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extend_start_hop_summary(start_hop_summary, log)\n\u001b[0;32m---> 43\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_extend_instance_summary\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstance_summary\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m start_hop_summary, instance_summary\n",
      "File \u001b[0;32m~/Documents/multi-x-serverless/multi_x_serverless/data_collector/components/workflow/workflow_retriever.py:68\u001b[0m, in \u001b[0;36mWorkflowRetriever._extend_instance_summary\u001b[0;34m(self, instance_summary, log)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_execution_latencies(log, instance_summary)\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_region_to_region_transmission(log, instance_summary)\n\u001b[0;32m---> 68\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle_missing_region_to_region_transmission_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstance_summary\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_non_executions(log, instance_summary)\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_calculate_invocation_probability(instance_summary)\n",
      "File \u001b[0;32m~/Documents/multi-x-serverless/multi_x_serverless/data_collector/components/workflow/workflow_retriever.py:165\u001b[0m, in \u001b[0;36mWorkflowRetriever._handle_missing_region_to_region_transmission_data\u001b[0;34m(self, instance_summary)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m transfer_information \u001b[38;5;129;01min\u001b[39;00m to_regions\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[1;32m    164\u001b[0m     existing_sizes \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mfloat\u001b[39m(data) \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m transfer_information[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransfer_size_to_transfer_latencies\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mkeys()]\n\u001b[0;32m--> 165\u001b[0m     missing_sizes \u001b[38;5;241m=\u001b[39m \u001b[43mall_transfer_sizes\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mexisting_sizes\u001b[49m\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m missing_size \u001b[38;5;129;01min\u001b[39;00m missing_sizes:\n\u001b[1;32m    168\u001b[0m         \u001b[38;5;66;03m# Find the nearest size for which we have data\u001b[39;00m\n\u001b[1;32m    169\u001b[0m         nearest_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(\n\u001b[1;32m    170\u001b[0m             transfer_information[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransfer_size_to_transfer_latencies\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mkeys(),\n\u001b[1;32m    171\u001b[0m             key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x, missing_size\u001b[38;5;241m=\u001b[39mmissing_size: \u001b[38;5;28mabs\u001b[39m(\u001b[38;5;28mfloat\u001b[39m(x) \u001b[38;5;241m-\u001b[39m missing_size),  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    172\u001b[0m         )\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for -: 'set' and 'list'"
     ]
    }
   ],
   "source": [
    "experiments_collected_data_dir = f\"./data/collected_workflow/{workflow_id}/experiments\"\n",
    "\n",
    "workflow_collect(f\"./data/run_data/experiments/{workflow_id}/data.json\", experiments_collected_data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multi_x_serverless.routing.deployment_input.components.calculators.carbon_calculator import CarbonCalculator\n",
    "from multi_x_serverless.routing.deployment_input.components.loaders.carbon_loader import CarbonLoader\n",
    "from multi_x_serverless.routing.deployment_input.components.loaders.datacenter_loader import DatacenterLoader\n",
    "from multi_x_serverless.routing.deployment_input.components.calculators.runtime_calculator import RuntimeCalculator\n",
    "\n",
    "from multi_x_serverless.common.models.endpoints import Endpoints\n",
    "from multi_x_serverless.routing.deployment_input.components.loaders.workflow_loader import WorkflowLoader\n",
    "\n",
    "result_data_points = {}\n",
    "\n",
    "deployment_result_path = f'./data/deployment_results/{workflow_id}/'\n",
    "\n",
    "endpoints = Endpoints()\n",
    "\n",
    "def calculate_carbon_of_deployment_at_time(deployment, date, hour_of_day):\n",
    "\n",
    "    def mock_carbon_loader_setup(self, available_regions: set[str]):\n",
    "        real_available_regions = list(locations.keys())\n",
    "        loaded_carbon_data = {}\n",
    "        for region in real_available_regions:\n",
    "            carbon_data_path = f'./data/collected_carbon/{region}/{date}/data.json'\n",
    "            with open(carbon_data_path, 'r') as file:\n",
    "                loaded_carbon_data[f'aws:{region}'] = json.load(file)\n",
    "        self._carbon_data = loaded_carbon_data\n",
    "    \n",
    "    def mock_workflow_loader_setup(self, workflow_id: str):\n",
    "        collected_data_dir = experiments_collected_data_dir + f\"/data.json\"\n",
    "\n",
    "        with open(collected_data_dir, \"r\") as f:\n",
    "            self._workflow_data = json.load(f)\n",
    "\n",
    "    with patch.object(CarbonLoader, 'setup', new=mock_carbon_loader_setup), patch.object(WorkflowLoader, 'setup', new=mock_workflow_loader_setup):\n",
    "        carbon_loader = CarbonLoader(None)\n",
    "        datacenter_loader = DatacenterLoader(endpoints.get_data_collector_client())\n",
    "        workflow_loader = WorkflowLoader(None)\n",
    "        carbon_calculator = CarbonCalculator(carbon_loader, datacenter_loader, workflow_loader, None)\n",
    "\n",
    "        carbon_calculator.alter_carbon_setting(hour_of_day)\n",
    "\n",
    "        total_execution_carbon = 0\n",
    "        total_transmission_carbon = 0\n",
    "        for instance_name, provider_region in deployment.items():\n",
    "            total_execution_carbon_per_instance = 0\n",
    "            for execution_latency in workflow_loader._workflow_data[\"instance_summary\"][instance_name][\"executions\"][f\"{provider_region['provider']}_{provider_region['region']}\"]:\n",
    "                total_execution_carbon_per_instance += carbon_calculator.calculate_execution_carbon(instance_name, f\"{provider_region['provider']}_{provider_region['region']}\", execution_latency)\n",
    "\n",
    "            avg_execution_carbon_per_instance_for_this_day = total_execution_carbon_per_instance / len(workflow_loader._workflow_data[\"instance_summary\"][instance_name][\"executions\"][f\"{provider_region['provider']}_{provider_region['region']}\"])\n",
    "            total_execution_carbon += avg_execution_carbon_per_instance_for_this_day\n",
    "\n",
    "            for to_instance, to_instance_data in workflow_loader._workflow_data[\"instance_summary\"][instance_name][\"to_instance\"].items():\n",
    "                total_from_to_transfer_carbon = 0\n",
    "                number_of_data = 0\n",
    "                from_region = f\"{provider_region['provider']}_{provider_region['region']}\"\n",
    "                to_region = f\"{deployment[to_instance]['provider_region']['provider']}_{deployment[to_instance]['provider_region']['region']}\"\n",
    "                transfer_size_to_transfer_latencies = to_instance_data[\"regions_to_regions\"][from_region][to_region]\n",
    "                for transfer_size, transfer_latencies in transfer_size_to_transfer_latencies.items():\n",
    "                    for transfer_latency in transfer_latencies:\n",
    "                        total_from_to_transfer_carbon += carbon_calculator.calculate_transmission_carbon(from_region, to_region, float(transfer_size), transfer_latency)\n",
    "                        number_of_data += 1\n",
    "                avg_transfer_carbon_for_this_day = total_from_to_transfer_carbon / number_of_data\n",
    "            \n",
    "                total_transmission_carbon += avg_transfer_carbon_for_this_day\n",
    "        \n",
    "        return {\"total_execution_carbon\": total_execution_carbon, \"total_transmission_carbon\": total_transmission_carbon}\n",
    "\n",
    "for directory in os.listdir(deployment_result_path):\n",
    "    if not os.path.isdir(os.path.join(deployment_result_path, directory)):\n",
    "        continue\n",
    "    result_data_points[directory] = []\n",
    "\n",
    "    for file in os.listdir(os.path.join(deployment_result_path, directory)):\n",
    "        day_date = file.split(\".\")[0]\n",
    "\n",
    "        day_datetime = datetime.strptime(day_date, \"%Y-%m-%d\")\n",
    "\n",
    "        with open(os.path.join(deployment_result_path, directory, file), 'r') as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        for time_key, deployment in data[\"time_keys_to_staging_area_data\"].items():\n",
    "            calculated_carbon = calculate_carbon_of_deployment_at_time(deployment, day_date, time_key)\n",
    "        \n",
    "            result_data_points[directory].append({\n",
    "                \"time_key\": time_key,\n",
    "                \"carbon_intensity\": calculated_carbon[\"total_execution_carbon\"] + calculated_carbon[\"total_transmission_carbon\"],\n",
    "                \"total_execution_carbon\": calculated_carbon[\"total_execution_carbon\"],\n",
    "                \"total_transmission_carbon\": calculated_carbon[\"total_transmission_carbon\"]\n",
    "            })\n",
    "\n",
    "# Store the result data points\n",
    "with open(f'./data/deployment_results/{workflow_id}/result_data_points.json', 'w') as f:\n",
    "    json.dump(result_data_points, f, indent=4)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multi-x-serverless-RxvPqp3n-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
