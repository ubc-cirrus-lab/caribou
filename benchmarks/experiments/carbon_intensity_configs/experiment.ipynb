{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "from unittest.mock import patch\n",
    "import boto3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define locations\n",
    "locations = {\n",
    "    \"us-west-2\": (43.8041334, -120.5542012),\n",
    "    \"us-west-1\": (38.8375215, -120.8958242),\n",
    "}\n",
    "\n",
    "start_date = datetime(2023, 6, 1, 0, 0, 0)\n",
    "\n",
    "end_date = datetime(2023, 6, 30, 0, 0, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Carbon Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data combined and saved to ./data/carbon/us-west-2_carbon_data.json\n",
      "Data combined and saved to ./data/carbon/us-west-1_carbon_data.json\n"
     ]
    }
   ],
   "source": [
    "# Function to query the API\n",
    "def query_api(start, end, lat, lon, token):\n",
    "    url = f\"https://api-access.electricitymaps.com/free-tier/carbon-intensity/past-range?lat={lat}&lon={lon}&start={start}&end={end}\"\n",
    "    headers = {\"auth-token\": token}\n",
    "    response = requests.get(url, headers=headers, timeout=10)\n",
    "    if response.status_code == 200:\n",
    "        return response.json()[\"data\"]\n",
    "    else:\n",
    "        print(f\"Error querying API: {response.status_code}\")\n",
    "        return []\n",
    "\n",
    "def run():\n",
    "    for location, (latitude, longitude) in locations.items():\n",
    "        current_start = start_date - timedelta(days=7) # Start 7 days before the start date\n",
    "        combined_data = []\n",
    "\n",
    "        while current_start < end_date:\n",
    "            current_end = current_start + timedelta(days=10)\n",
    "            if current_end > end_date:\n",
    "                current_end = end_date\n",
    "            data = query_api(\n",
    "                current_start, current_end, latitude, longitude, os.environ.get(\"ELECTRICITY_MAPS_AUTH_TOKEN\")\n",
    "            )\n",
    "            combined_data.extend(data)\n",
    "            current_start = current_end + timedelta(days=1)\n",
    "\n",
    "        # Save the combined data to a file\n",
    "        output_file = f\"./data/carbon/{location}_carbon_data.json\"\n",
    "        with open(output_file, \"w\") as f:\n",
    "            json.dump(combined_data, f)\n",
    "\n",
    "        print(f\"Data combined and saved to {output_file}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Carbon Collector Logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "from collections import defaultdict\n",
    "\n",
    "def calculate_distance(lat1, lon1, lat2, lon2):\n",
    "    r = 6371.0  # Earth radius in kilometers\n",
    "    dlat = math.radians(lat2 - lat1)\n",
    "    dlon = math.radians(lon2 - lon1)\n",
    "    a = math.sin(dlat / 2)**2 + math.cos(math.radians(lat1)) * math.cos(math.radians(lat2)) * math.sin(dlon / 2)**2\n",
    "    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))\n",
    "    return r * c\n",
    "\n",
    "def process_and_store_carbon_data_for_regions(input_file, output_folder, regions_info, current_region):\n",
    "    os.makedirs(output_folder, exist_ok=True)  # Ensure output directory exists\n",
    "    \n",
    "    with open(input_file, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    \n",
    "    # For each day between start_date and end_date\n",
    "    for day in range((end_date - start_date).days + 1):\n",
    "        current_date = start_date + timedelta(days=day)\n",
    "\n",
    "        # Get all the data for the previous 7 days\n",
    "        previous_7_days_data = [entry for entry in data if datetime.strptime(entry['datetime'], '%Y-%m-%dT%H:%M:%S.%fZ').date() >= (current_date - timedelta(days=7)).date() and datetime.strptime(entry['datetime'], '%Y-%m-%dT%H:%M:%S.%fZ').date() < current_date.date()]\n",
    "\n",
    "        # Calculate the average carbon intensity for the previous 7 days\n",
    "        if not previous_7_days_data:\n",
    "            continue\n",
    "        overall_sum = sum(item['carbonIntensity'] for item in previous_7_days_data)\n",
    "        overall_avg = overall_sum / len(previous_7_days_data)\n",
    "\n",
    "        # Calculate the average carbon intensity for each hour of the day\n",
    "        hourly_averages = defaultdict(list)\n",
    "        for item in previous_7_days_data:\n",
    "            hour = datetime.strptime(item['datetime'], '%Y-%m-%dT%H:%M:%S.%fZ').hour\n",
    "            hourly_averages[hour].append(item['carbonIntensity'])\n",
    "\n",
    "        hourly_avg = {hour: sum(values) / len(values) for hour, values in hourly_averages.items()}\n",
    "\n",
    "        # Calculate the distances between the regions\n",
    "        transmission_distances = {\n",
    "            f\"aws:{region_key}\": calculate_distance(regions_info[current_region][0], regions_info[current_region][1], regions_info[region_key][0], regions_info[region_key][1]) for region_key in regions_info\n",
    "        }\n",
    "    \n",
    "        # Assemble the result dictionary\n",
    "        result_dict = {\n",
    "            \"averages\": {\n",
    "                \"overall\": {\"carbon_intensity\": overall_avg},\n",
    "                **{str(hour): {\"carbon_intensity\": avg} for hour, avg in hourly_avg.items()}\n",
    "            },\n",
    "            \"units\": \"gCO2eq/kWh\",\n",
    "            \"transmission_distances\": transmission_distances\n",
    "        }\n",
    "\n",
    "        # Store the result\n",
    "        day_folder = os.path.join(output_folder, current_date.strftime('%Y-%m-%d'))\n",
    "        os.makedirs(day_folder, exist_ok=True)\n",
    "        with open(os.path.join(day_folder, 'data.json'), 'w') as outfile:\n",
    "            json.dump(result_dict, outfile, indent=4)\n",
    "\n",
    "for region in locations:\n",
    "    process_and_store_carbon_data_for_regions(f\"./data/carbon/{region}_carbon_data.json\", f\"./data/collected_carbon/{region}\", locations, region)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Data from the Home Region\n",
    "\n",
    "For each benchmark:\n",
    "1. Deploy the benchmark (optimally without any constraints) to the home region (which should be one of the regions we eventually also want to take into consideration).\n",
    "2. Run the benchmark for a time (use the `invoke_serverless_function_uniform.sh` script to do so). I used it for example like this:\n",
    "```bash\n",
    "poetry run ./invoke_serverless_function_uniform.sh dna_visualization 0.0.1 200 3600\n",
    "```\n",
    "3. Use the following small script to log sync the data and store it locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the benchmarks:\n",
    "benchmarks = [\"dna_visualization-0.0.1\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Log Sync"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multi_x_serverless.syncers.log_syncer import LogSyncWorkflow\n",
    "from multi_x_serverless.common.models.endpoints import Endpoints\n",
    "from multi_x_serverless.common.constants import DEPLOYMENT_MANAGER_RESOURCE_TABLE, GLOBAL_TIME_ZONE\n",
    "\n",
    "# This assumes that the run happened in the last 24 hours\n",
    "time_intervals = [(datetime.now(GLOBAL_TIME_ZONE) - timedelta(days=1), datetime.now(GLOBAL_TIME_ZONE))]\n",
    "\n",
    "region_clients = {}\n",
    "endpoints = Endpoints()\n",
    "\n",
    "for workflow_id in benchmarks:\n",
    "    deployment_manager_config_str = endpoints.get_deployment_manager_client().get_value_from_table(\n",
    "        DEPLOYMENT_MANAGER_RESOURCE_TABLE, workflow_id\n",
    "    )\n",
    "\n",
    "    raw_data_dir = f\"./data/home_region_run_data/{workflow_id}\"\n",
    "    os.makedirs(raw_data_dir, exist_ok=True)\n",
    "\n",
    "    def mock_upload_data(self, data_for_upload: str):\n",
    "        with open(f\"{raw_data_dir}/data.json\", \"w\") as f:\n",
    "            f.write(data_for_upload)\n",
    "\n",
    "    with patch.object(LogSyncWorkflow, \"_upload_data\", mock_upload_data):\n",
    "        log_sync_workflow = LogSyncWorkflow(\n",
    "            workflow_id,\n",
    "            region_clients,\n",
    "            deployment_manager_config_str,\n",
    "            time_intervals,\n",
    "            endpoints.get_datastore_client(),\n",
    "            {}\n",
    "        )\n",
    "        log_sync_workflow.sync_workflow()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Workflow Collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multi_x_serverless.data_collector.components.workflow.workflow_retriever import WorkflowRetriever\n",
    "from multi_x_serverless.data_collector.components.data_collector import DataCollector\n",
    "\n",
    "workflow_retriever = WorkflowRetriever(None)\n",
    "\n",
    "for workflow_id in benchmarks:\n",
    "\n",
    "    raw_data_dir = f\"./data/home_region_run_data/{workflow_id}/data.json\"\n",
    "\n",
    "    with open(raw_data_dir, \"r\") as f:\n",
    "        data = f.read()\n",
    "\n",
    "    workflow_summary = workflow_retriever._transform_workflow_summary(\n",
    "        data,\n",
    "    )\n",
    "\n",
    "    collected_data_dir = f\"./data/collected_workflow/{workflow_id}/home_region\"\n",
    "    os.makedirs(collected_data_dir, exist_ok=True)\n",
    "\n",
    "    with open(f\"{collected_data_dir}/data.json\", \"w\") as f:\n",
    "        json.dump(workflow_summary, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the Deployment Algorithm\n",
    "\n",
    "We need to run the deployment algorithm for the days between the start and end date. We will run it every day and store the results locally in files.\n",
    "Additionally, we need to provide the data collected by a specific workflow as the input to the workflow loader.\n",
    "\n",
    "**TODO BEFORE the following is executed:** For each of the involved benchmarks we need to actually have some data from the home region. So please do **Get Data from the Home Region** first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multi_x_serverless.common.constants import GLOBAL_SYSTEM_REGION, DEPLOYMENT_OPTIMIZATION_MONITOR_RESOURCE_TABLE\n",
    "from multi_x_serverless.routing.workflow_config import WorkflowConfig\n",
    "from multi_x_serverless.routing.deployment_algorithms.stochastic_heuristic_deployment_algorithm import StochasticHeuristicDeploymentAlgorithm\n",
    "from multi_x_serverless.routing.deployment_input.components.loaders.carbon_loader import CarbonLoader\n",
    "from multi_x_serverless.routing.deployment_input.components.loaders.region_viability_loader import RegionViabilityLoader\n",
    "from multi_x_serverless.routing.deployment_algorithms.deployment_algorithm import DeploymentAlgorithm\n",
    "from multi_x_serverless.routing.deployment_input.components.loaders.workflow_loader import WorkflowLoader\n",
    "\n",
    "# Define the constraint configurations\n",
    "constraint_configurations = {\n",
    "    \"no_constraints\": {\n",
    "        \"constraints\": {\n",
    "            \"hard_resource_constraints\": {\"cost\": None, \"runtime\": None, \"carbon\": None},\n",
    "            \"soft_resource_constraints\": {\"cost\": None, \"runtime\": None, \"carbon\": None},\n",
    "            \"priority_order\": [\"carbon\", \"cost\", \"runtime\"],\n",
    "        }\n",
    "    },\n",
    "    \"five_percent_runtime_constraint\": {\n",
    "        \"constraints\": {\n",
    "            \"hard_resource_constraints\": {\"cost\": None, \"runtime\": {\"type\": \"relative\", \"value\": 105}, \"carbon\": None},\n",
    "            \"soft_resource_constraints\": {\"cost\": None, \"runtime\": None, \"carbon\": None},\n",
    "            \"priority_order\": [\"carbon\", \"cost\", \"runtime\"],\n",
    "        }\n",
    "    },\n",
    "}\n",
    "\n",
    "dynamodb_client = boto3.client('dynamodb', region_name=GLOBAL_SYSTEM_REGION)\n",
    "for benchmark in benchmarks:\n",
    "    response = dynamodb_client.get_item(TableName=DEPLOYMENT_OPTIMIZATION_MONITOR_RESOURCE_TABLE, Key={\"key\": {\"S\": benchmark}})\n",
    "    item = response.get(\"Item\")\n",
    "    workflow_config_from_table = item[\"value\"][\"S\"]\n",
    "\n",
    "    workflow_config_dict = json.loads(json.loads(workflow_config_from_table).get(\"workflow_config\"))\n",
    "    for configuration_name, configuration in constraint_configurations.items():\n",
    "        workflow_config_dict[\"constraints\"] = configuration[\"constraints\"]\n",
    "        workflow_config = WorkflowConfig(workflow_config_dict)\n",
    "\n",
    "        for day in range((end_date - start_date).days + 1):\n",
    "            current_date = start_date + timedelta(days=day)\n",
    "\n",
    "            def mock_carbon_loader_setup(self, available_regions: set[str]):\n",
    "                real_available_regions = list(locations.keys())\n",
    "                loaded_carbon_data = {}\n",
    "                for region in real_available_regions:\n",
    "                    carbon_data_path = f'./data/collected_carbon/{region}/{current_date.strftime(\"%Y-%m-%d\")}/data.json'\n",
    "                    with open(carbon_data_path, 'r') as file:\n",
    "                        loaded_carbon_data[f'aws:{region}'] = json.load(file)\n",
    "                self._carbon_data = loaded_carbon_data\n",
    "\n",
    "            def mock_workflow_loader_setup(self, workflow_id: str):\n",
    "                collected_data_dir = f\"./data/collected_workflow/{workflow_id}/home_region/data.json\"\n",
    "\n",
    "                with open(collected_data_dir, \"r\") as f:\n",
    "                    self._workflow_data = json.load(f)\n",
    "\n",
    "            def mock_region_viability_setup(self):\n",
    "                self._available_regions = [f'aws:{region}' for region in locations.keys()]\n",
    "\n",
    "            def mock_upload_result(self, result: dict):\n",
    "                result_path = f'./data/deployment_results/{benchmark}/{configuration_name}/'\n",
    "                os.makedirs(result_path, exist_ok=True)\n",
    "                with open(os.path.join(result_path, f'{current_date.strftime(\"%Y-%m-%d\")}.json'), 'w') as file:\n",
    "                    json.dump(result, file, indent=4)\n",
    "\n",
    "            with patch.object(CarbonLoader, 'setup', new=mock_carbon_loader_setup), patch.object(RegionViabilityLoader, 'setup', new=mock_region_viability_setup), patch.object(DeploymentAlgorithm, '_upload_result', new=mock_upload_result), patch.object(WorkflowLoader, 'setup', new=mock_workflow_loader_setup):\n",
    "                algorithm = StochasticHeuristicDeploymentAlgorithm(workflow_config)\n",
    "                algorithm.run([f\"{i}\" for i in range(24)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run new experiments for the new deployments\n",
    "\n",
    "Now since we have the new deployments, we can run the experiments for them. We don't need to run all experiments, basically we need to run all the combinations of the regions involved. For this we need to iterate over the potential deployments, find combinations of regions that are not yet collected and run the experiments for them. The following script will do this on a per benchmark basis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Please select the benchmark you want to run the experiment for\n",
    "benchmark = benchmarks[0]\n",
    "\n",
    "deployment_result_path = f'./data/deployment_results/{benchmark}'\n",
    "\n",
    "unique_instance_region_combinations = set()\n",
    "\n",
    "# Iterate over all results, retrieving the combinations of instances and regions\n",
    "for directory in os.listdir(deployment_result_path):\n",
    "    if not os.path.isdir(os.path.join(deployment_result_path, directory)):\n",
    "        continue\n",
    "    for file in os.listdir(os.path.join(deployment_result_path, directory)):\n",
    "        with open(os.path.join(deployment_result_path, directory, file), 'r') as f:\n",
    "            data = json.load(f)\n",
    "        for time_key, deployment in data[\"time_keys_to_staging_area_data\"].items():\n",
    "            deployment_str = json.dumps(deployment)\n",
    "            unique_instance_region_combinations.add(deployment_str)\n",
    "\n",
    "# Store the unique combinations\n",
    "with open(f'./data/deployment_results/{benchmark}/unique_instance_region_combinations.json', 'w') as f:\n",
    "    json.dump(list(unique_instance_region_combinations), f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Re-deploy all deployments\n",
    "\n",
    "Before we can run the experiments we need the code to be in the right regions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found credentials in shared credentials file: ~/.aws/credentials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found credentials in shared credentials file: ~/.aws/credentials\n",
      "Found credentials in shared credentials file: ~/.aws/credentials\n",
      "Found credentials in shared credentials file: ~/.aws/credentials\n"
     ]
    }
   ],
   "source": [
    "from multi_x_serverless.deployment.common.deploy.deployer import Deployer, create_default_deployer\n",
    "from multi_x_serverless.common.constants import (\n",
    "    DEPLOYMENT_MANAGER_RESOURCE_TABLE\n",
    ")\n",
    "from multi_x_serverless.deployment.common.factories.deployer_factory import DeployerFactory\n",
    "\n",
    "endpoints = Endpoints()\n",
    "\n",
    "workflow_id = benchmarks[0]\n",
    "\n",
    "deployment_result_path = f'./data/deployment_results/{workflow_id}/unique_instance_region_combinations.json'\n",
    "\n",
    "workflow_data_raw = endpoints.get_deployment_manager_client().get_value_from_table(\n",
    "    DEPLOYMENT_MANAGER_RESOURCE_TABLE, workflow_id\n",
    ")\n",
    "\n",
    "workflow_data = json.loads(workflow_data_raw)\n",
    "\n",
    "workflow_function_descriptions = json.loads(workflow_data[\"workflow_function_descriptions\"])\n",
    "deployment_config = json.loads(workflow_data[\"deployment_config\"])\n",
    "deployed_regions = json.loads(workflow_data[\"deployed_regions\"])\n",
    "\n",
    "deployer_factory = DeployerFactory(project_dir=None)\n",
    "config = deployer_factory.create_config_obj_from_dict(deployment_config=deployment_config)\n",
    "deployer = create_default_deployer(config)\n",
    "\n",
    "with open(deployment_result_path, 'r') as f:\n",
    "    unique_instance_region_combinations = json.load(f)\n",
    "\n",
    "for combination in unique_instance_region_combinations:\n",
    "    deployment = json.loads(combination)\n",
    "\n",
    "    deployer.re_deploy(\n",
    "        workflow_function_descriptions,\n",
    "        deployed_regions,\n",
    "        deployment\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multi-x-serverless-RxvPqp3n-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
