{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "from unittest.mock import patch\n",
    "import boto3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define locations\n",
    "locations = {\n",
    "    \"us-west-2\": (43.8041334, -120.5542012),\n",
    "    \"us-west-1\": (38.8375215, -120.8958242),\n",
    "}\n",
    "\n",
    "start_date = datetime(2023, 6, 1, 0, 0, 0)\n",
    "\n",
    "end_date = datetime(2023, 6, 30, 0, 0, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Carbon Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data combined and saved to ./data/carbon/us-west-2_carbon_data.json\n",
      "Data combined and saved to ./data/carbon/us-west-1_carbon_data.json\n"
     ]
    }
   ],
   "source": [
    "# Function to query the API\n",
    "def query_api(start, end, lat, lon, token):\n",
    "    url = f\"https://api-access.electricitymaps.com/free-tier/carbon-intensity/past-range?lat={lat}&lon={lon}&start={start}&end={end}\"\n",
    "    headers = {\"auth-token\": token}\n",
    "    response = requests.get(url, headers=headers, timeout=10)\n",
    "    if response.status_code == 200:\n",
    "        return response.json()[\"data\"]\n",
    "    else:\n",
    "        print(f\"Error querying API: {response.status_code}\")\n",
    "        return []\n",
    "\n",
    "def run():\n",
    "    for location, (latitude, longitude) in locations.items():\n",
    "        current_start = start_date - timedelta(days=7) # Start 7 days before the start date\n",
    "        combined_data = []\n",
    "\n",
    "        while current_start < end_date:\n",
    "            current_end = current_start + timedelta(days=10)\n",
    "            if current_end > end_date:\n",
    "                current_end = end_date\n",
    "            data = query_api(\n",
    "                current_start, current_end, latitude, longitude, os.environ.get(\"ELECTRICITY_MAPS_AUTH_TOKEN\")\n",
    "            )\n",
    "            combined_data.extend(data)\n",
    "            current_start = current_end + timedelta(days=1)\n",
    "\n",
    "        # Save the combined data to a file\n",
    "        output_file = f\"./data/carbon/{location}_carbon_data.json\"\n",
    "        with open(output_file, \"w\") as f:\n",
    "            json.dump(combined_data, f)\n",
    "\n",
    "        print(f\"Data combined and saved to {output_file}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Carbon Collector Logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "from collections import defaultdict\n",
    "\n",
    "def calculate_distance(lat1, lon1, lat2, lon2):\n",
    "    r = 6371.0  # Earth radius in kilometers\n",
    "    dlat = math.radians(lat2 - lat1)\n",
    "    dlon = math.radians(lon2 - lon1)\n",
    "    a = math.sin(dlat / 2)**2 + math.cos(math.radians(lat1)) * math.cos(math.radians(lat2)) * math.sin(dlon / 2)**2\n",
    "    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))\n",
    "    return r * c\n",
    "\n",
    "def process_and_store_carbon_data_for_regions(input_file, output_folder, regions_info, current_region):\n",
    "    os.makedirs(output_folder, exist_ok=True)  # Ensure output directory exists\n",
    "    \n",
    "    with open(input_file, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    \n",
    "    # For each day between start_date and end_date\n",
    "    for day in range((end_date - start_date).days + 1):\n",
    "        current_date = start_date + timedelta(days=day)\n",
    "\n",
    "        # Get all the data for the previous 7 days\n",
    "        previous_7_days_data = [entry for entry in data if datetime.strptime(entry['datetime'], '%Y-%m-%dT%H:%M:%S.%fZ').date() >= (current_date - timedelta(days=7)).date() and datetime.strptime(entry['datetime'], '%Y-%m-%dT%H:%M:%S.%fZ').date() < current_date.date()]\n",
    "\n",
    "        # Calculate the average carbon intensity for the previous 7 days\n",
    "        if not previous_7_days_data:\n",
    "            continue\n",
    "        overall_sum = sum(item['carbonIntensity'] for item in previous_7_days_data)\n",
    "        overall_avg = overall_sum / len(previous_7_days_data)\n",
    "\n",
    "        # Calculate the average carbon intensity for each hour of the day\n",
    "        hourly_averages = defaultdict(list)\n",
    "        for item in previous_7_days_data:\n",
    "            hour = datetime.strptime(item['datetime'], '%Y-%m-%dT%H:%M:%S.%fZ').hour\n",
    "            hourly_averages[hour].append(item['carbonIntensity'])\n",
    "\n",
    "        hourly_avg = {hour: sum(values) / len(values) for hour, values in hourly_averages.items()}\n",
    "\n",
    "        # Calculate the distances between the regions\n",
    "        transmission_distances = {\n",
    "            f\"aws:{region_key}\": calculate_distance(regions_info[current_region][0], regions_info[current_region][1], regions_info[region_key][0], regions_info[region_key][1]) for region_key in regions_info\n",
    "        }\n",
    "    \n",
    "        # Assemble the result dictionary\n",
    "        result_dict = {\n",
    "            \"averages\": {\n",
    "                \"overall\": {\"carbon_intensity\": overall_avg},\n",
    "                **{str(hour): {\"carbon_intensity\": avg} for hour, avg in hourly_avg.items()}\n",
    "            },\n",
    "            \"units\": \"gCO2eq/kWh\",\n",
    "            \"transmission_distances\": transmission_distances\n",
    "        }\n",
    "\n",
    "        # Store the result\n",
    "        day_folder = os.path.join(output_folder, current_date.strftime('%Y-%m-%d'))\n",
    "        os.makedirs(day_folder, exist_ok=True)\n",
    "        with open(os.path.join(day_folder, 'data.json'), 'w') as outfile:\n",
    "            json.dump(result_dict, outfile, indent=4)\n",
    "\n",
    "for region in locations:\n",
    "    process_and_store_carbon_data_for_regions(f\"./data/carbon/{region}_carbon_data.json\", f\"./data/collected_carbon/{region}\", locations, region)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the Deployment Algorithm\n",
    "\n",
    "We need to run the deployment algorithm for the days between the start and end date. We will run it every day and store the results locally in files.\n",
    "Additionally, we need to provide the data collected by a specific workflow as the input to the workflow loader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multi_x_serverless.common.constants import GLOBAL_SYSTEM_REGION, DEPLOYMENT_OPTIMIZATION_MONITOR_RESOURCE_TABLE\n",
    "from multi_x_serverless.routing.workflow_config import WorkflowConfig\n",
    "from multi_x_serverless.routing.deployment_algorithms.stochastic_heuristic_deployment_algorithm import StochasticHeuristicDeploymentAlgorithm\n",
    "from multi_x_serverless.routing.deployment_input.components.loaders.carbon_loader import CarbonLoader\n",
    "from multi_x_serverless.routing.deployment_input.components.loaders.region_viability_loader import RegionViabilityLoader\n",
    "from multi_x_serverless.routing.deployment_algorithms.deployment_algorithm import DeploymentAlgorithm\n",
    "\n",
    "# First define the involved benchmarks, these should already be deployed and have workflow data\n",
    "benchmarks = [\"dna_visualization-0.0.1\"]\n",
    "\n",
    "constraint_configurations = {\n",
    "    \"no_constraints\": {\n",
    "        \"constraints\": {\n",
    "            \"hard_resource_constraints\": {\"cost\": None, \"runtime\": None, \"carbon\": None},\n",
    "            \"soft_resource_constraints\": {\"cost\": None, \"runtime\": None, \"carbon\": None},\n",
    "            \"priority_order\": [\"carbon\", \"cost\", \"runtime\"],\n",
    "        }\n",
    "    },\n",
    "    \"five_percent_runtime_constraint\": {\n",
    "        \"constraints\": {\n",
    "            \"hard_resource_constraints\": {\"cost\": None, \"runtime\": {\"type\": \"relative\", \"value\": 105}, \"carbon\": None},\n",
    "            \"soft_resource_constraints\": {\"cost\": None, \"runtime\": None, \"carbon\": None},\n",
    "            \"priority_order\": [\"carbon\", \"cost\", \"runtime\"],\n",
    "        }\n",
    "    },\n",
    "}\n",
    "\n",
    "dynamodb_client = boto3.client('dynamodb', region_name=GLOBAL_SYSTEM_REGION)\n",
    "for benchmark in benchmarks:\n",
    "    response = dynamodb_client.get_item(TableName=DEPLOYMENT_OPTIMIZATION_MONITOR_RESOURCE_TABLE, Key={\"key\": {\"S\": benchmark}})\n",
    "    item = response.get(\"Item\")\n",
    "    workflow_config_from_table = item[\"value\"][\"S\"]\n",
    "\n",
    "    workflow_config_dict = json.loads(json.loads(workflow_config_from_table).get(\"workflow_config\"))\n",
    "    for configuration_name, configuration in constraint_configurations.items():\n",
    "        workflow_config_dict[\"constraints\"] = configuration[\"constraints\"]\n",
    "        workflow_config = WorkflowConfig(workflow_config_dict)\n",
    "\n",
    "        for day in range((end_date - start_date).days + 1):\n",
    "            current_date = start_date + timedelta(days=day)\n",
    "\n",
    "            def mock_carbon_loader_setup(self, available_regions: set[str]):\n",
    "                real_available_regions = list(locations.keys())\n",
    "                loaded_carbon_data = {}\n",
    "                for region in real_available_regions:\n",
    "                    carbon_data_path = f'./data/collected_carbon/{region}/{current_date.strftime(\"%Y-%m-%d\")}/data.json'\n",
    "                    with open(carbon_data_path, 'r') as file:\n",
    "                        loaded_carbon_data[f'aws:{region}'] = json.load(file)\n",
    "                self._carbon_data = loaded_carbon_data\n",
    "\n",
    "            def mock_region_viability_setup(self):\n",
    "                self._available_regions = [f'aws:{region}' for region in locations.keys()]\n",
    "\n",
    "            def mock_upload_result(self, result: dict):\n",
    "                result_path = f'./data/deployment_results/{benchmark}/{configuration_name}/'\n",
    "                os.makedirs(result_path, exist_ok=True)\n",
    "                with open(os.path.join(result_path, f'{current_date.strftime(\"%Y-%m-%d\")}.json'), 'w') as file:\n",
    "                    json.dump(result, file, indent=4)\n",
    "\n",
    "            with patch.object(CarbonLoader, 'setup', new=mock_carbon_loader_setup), patch.object(RegionViabilityLoader, 'setup', new=mock_region_viability_setup), patch.object(DeploymentAlgorithm, '_upload_result', new=mock_upload_result):\n",
    "                algorithm = StochasticHeuristicDeploymentAlgorithm(workflow_config)\n",
    "                algorithm.run([f\"{i}\" for i in range(24)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multi-x-serverless-RxvPqp3n-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
